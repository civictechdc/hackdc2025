Column 1,Project,Evaluator,Impact & Relevance,Novelty,Amplification,Open Source Practices,Usability & Design,Continuity Potential,Score,Comments,
,canOfSpam,Fred Trotter,3,3,3,2,3,3,2.85,Good stab at the problem!,
,canOfSpam,Ben Coleman,4.5,3,3,2,3,3,3.15,"This project was one that we put out to the groups, and I was pleased to see someone explore the topic. There was minimal documentation. The code contains developer-specific paths, making it hard to re-use. From the presentation, they used a hash of text rather than a full analysis of timestamps, which limits the approach to showing only exact duplicates, and an extension to consider similarity would be helpful. There is good potential for a dashboard to show unique comments and the number in each duplicate group.",
,canOfSpam,Santhosh kumar Veeramalla,4,2.5,2.5,3,2,3,2.88,"The core idea is good, but the approach needs significant changes to support large datasets. The comparison uses O(n^2) sequential processing, which is not suitable for large datasets and reduces performance. Hardcoded values, such as the similarity threshold, should be parameterized for flexibility and reusability. There is no mention of lowercase, uppercase, or stop word removal, which can reduce accuracy, and there is a lack of error handling. It can be further enhanced to support multi-language comment similarity checks.",
,canOfSpam,Gautami Nadkarni,3,3,3,3,3,3,3,"Documentation could be better, and more technical capabilities would have been nice.",
,canOfSpam,Michael Deeb,4,3,3,3,2,3,3.05,"Solid proof-of-concept for detecting coordinated bot comments using temporal bursts and duplicate detection; high civic value, moderate novelty.",
Total,canOfSpam,,4,3,3,3,3,3,3,MEDIAN,
,entity-resolution-team,Fred Trotter,3,2,3,2,2,2,2.35,Regex is a perfect hackathon strategy: 80% and quick!,
,entity-resolution-team,Ben Coleman,4,3,2,1,2,2,2.45,The code is in a Jupyter notebook with no supporting files. There is no documentation. They looked for emails using regex and grouped them by domain. It's an interesting way to identify organizations that submit multiple comments that are not duplicates.,
,entity-resolution-team,Santhosh kumar Veeramalla,2,2,2,1.5,2,1.5,1.85,"It lacks clear documentation, making it hard to understand and reuse. The code is not production-ready as it currently exists in a Jupyter notebook. Adding meaningful comments to explain the logic would greatly improve readability. The idea is valuable for analyzing organizational comments, but the current implementation makes it hard to adopt.",
,entity-resolution-team,Gautami Nadkarni,3.5,2.5,3,3,2.5,3,2.93,Poor documentation but a good idea.,
,entity-resolution-team,Melanie Kourbage,5,3,5,5,3,3,4,"Identifying the submitting organization may be challenging depending on the format of the comments that were submitted. In the example, ""Peg"" was identified as both CMS and QUA, Inc., even though ""Pam"" was responding to a CMS proposed rule. She would like to see more examples of how this could be leveraged successfully. She would use this to compare against a list of her member organizations (public health labs), partner organizations, and key industry partners to see which ones submitted a response. She sees a lot of potential here. The lower scores are only because she couldn't find the documentation for what comes next and is not clear on how the organization information is being identified from the comment.",
,entity-resolution-team,Michael Deeb,4,2,3,3,2,3,2.85,Provides a classical approach to grouping regulatory comments by inferred organization. Uses standard text-cleaning and matching techniques that work but are not groundbreaking. Code reproducible via notebook.,
Total,entity-resolution-team,,3.75,2.25,3,2.5,2,2.5,2.65,MEDIAN,
,hive-partitioned-parquet,Fred Trotter,5,5,4,5,3,4,4.4,Totally made me rethink how to use S3 as a database. Before... it was a metaphor.,
,hive-partitioned-parquet,Ben Coleman,5,4,5,4,3,3,4.05,"This project has significant potential for the mirrulations ecosystem. Since the event, he used ideas from this project to explore how to store a parquet representation of the data in the S3 bucket. The actual code in the project is limited, but the idea here is incredibly valuable.",
,hive-partitioned-parquet,Santhosh kumar Veeramalla,5,4,4,3,3,4,3.9,"The core idea is very solid, and the use of parquet and Hive partitioning is a smart and scalable approach. Choosing DuckDB is a good decision for lightweight use. The inclusion of a demo notebook provides clear value and practical demonstration. This approach is good for small and medium data but will not work for large-scale data. It lacks schema evolution and advanced features already available in the market, such as Delta Lake/Apache Iceberg, which handle Time Travel, schema evolution, and ACID transactions.",
,hive-partitioned-parquet,Evan Tung,4,4,5,3,2,3,3.55,"The speed of the queries executed on the Hive-partitioned parquet tables definitely stood out. During the live demo, it was amazing to see how the query could be executed against a remote S3 bucket without having to download the entire dataset. The README instructions are pretty well written, so any technical person could continue this without any problem. However, it would be pretty difficult for a non-technical person to use this.",
,hive-partitioned-parquet,Gautami Nadkarni,4,4,3,4,4,4.5,3.93,"Very well explained, one of the best documentations and scripting.",
,hive-partitioned-parquet,Michael Deeb,4,4,4,4,3,3,3.7,"Converts Mirrulations JSON to Hive-partitioned Parquet, sharply reducing query costs and enabling SQL access via DuckDB. Documentation and notebooks are clear and reproducible, and the technique can scale across agencies. Novelty is high because it applies a lesser known aspect of data-lake patterns to make the data more accessible. The Amplification affect is high, as this is a foundational tool for the mirrulations ecosystem.",
Total,hive-partitioned-parquet,,4.5,4,4,4,3,3.5,3.915,MEDIAN,
,llmgov,Fred Trotter,5,4,4,5,4,4,4.35,LLM that scales. Sign me up.,
,llmgov,Ben Coleman,5,5,5,4,4,4,4.55,"The RAG transformation process is another incredibly valuable contribution to the mirrulations ecosystem. The code to transform data into vector embeddings could readily be incorporated into the mirrulations ETL process. The example LLM queries are instructive, and he is curious how well this will scale to all the data.",
,llmgov,Santhosh kumar Veeramalla,5,4,4,4.5,4,4,4.28,"Leveraging RAG with LLMs and Vector embeddings is a powerful and scalable approach. It has clear documentation and clean code, making it easy to reuse. The smart use of cloud embeddings in S3 storage is cost-effective. Further enhancements will be needed, such as access control if sensitive or legally regulated content is involved.",
,llmgov,Evan Tung,5,2,4,3,5,3,3.65,"He really likes how usable this project was for non-technical users, and the fact that example questions were added helps that factor even more. He can definitely foresee the general public using this to ask questions about recent regulations. However, a RAG-based chatbot is one of the more standard applications of generative AI, and he would have liked to see some more creative applications of it. As it currently stands, the user also has to know how to run a set of bespoke CLI scripts in the correct order to run the CMS chatbot, so he would like to see that maintainability aspect improved.",
,llmgov,Gautami Nadkarni,3,3,3,3,3,3.5,3.08,"Average idea, documentation could be better.",
,llmgov,Taylor Wilson,5,4,5,5,4,5,4.65,"She works a lot with this kind of architecture in the context of known documentation sets. She thinks this is a natural place to go for public comment analysis because semantic search capability will supercharge the ability of users to interface with the content. She is curious if the more challenging parts of these data are vectorizable in a consistent and useful way, such as public comments that contain data tables or relevant numeric information. She asks what sort of preprocessing might be employed to identify the public comments that are most interoperable with this technology stack, and what to do with public comments that will pollute the vector database and draw down the quality of the semantic search at large.",
,llmgov,Michael Deeb,4,4,3,3,3,3,3.4,"Makes CMS regulations searchable via a RAG chat interface backed by an AWS Bedrock embeddings pipeline. High civic value and user-friendly Streamlit UI; code is modular with a pyproject and clear docs. Innovation is moderate given many LLM-RAG examples, was impressed with how they approached the overall data, and their prompt. The use of AWS-specific services is a bit limiting with the S3 Vector Store, but ultimately that is not a limited factor to the overall re-use of the code. The amplification affect is moderate, creating an embedding structure could facilitate other types of analysis.",
Total,llmgov,,5,4,4,4,4,4,4.28,MEDIAN,
,mirrulations-cli,Fred Trotter,5,4,5,5,5,5,4.8,"Another example of ""oh this is the right way to think about this"".",
,mirrulations-cli,Ben Coleman,4,3,5,5,4,3,3.95,This project was a wonderful extension of the work he did in preparation for the event. It makes the mirrulations-fetch and mirrulations-csv tools easier to obtain and use. He will incorporate this approach with the tools (Jay and he plan to work together to transfer ownership of the PyPi package).,
,mirrulations-cli,Santhosh kumar Veeramalla,4.5,4,4.5,4.5,4,4,4.25,"This is a practical and developer-friendly solution with strong real-world utility. It has clear documentation, quality code, and a streamlined workflow. With minor enhancements such as download optimizations and support for multiple output formats, it could become the default tool for working with Mirrulations data.",
,mirrulations-cli,Gautami Nadkarni,5,4.5,5,4,4,4,4.45,"This project excels in making a valuable but hard-to-access dataset easily usable. By thoughtfully combining and packaging existing scripts for distribution on PyPI, the team has delivered a highly practical and user-friendly solution.",
,mirrulations-cli,Taylor Wilson,5,3,5,5,5,3,4.3,"She really loves a quick, useful utility that expands the functionality of an existing tool and thinks contributions like this should be highly valued.",
,mirrulations-cli,Michael Deeb,5,2,2,5,5,5,3.95,"Packaging Mirrulations fetch and CSV utilities into a single, PyPI-distributed CLI markedly lowers the barrier to accessing regulatory data and should see quick adoption. Strong licensing, changelog, tests, and Click-based UX merit high marks. Low Novelty, but sometimes the small things are the most impactful. Limited amplification affect since this is more a tool to be used by others initially exploring this data, or accessing specific dockets.",
Total,mirrulations-cli,,5,3.5,5,5,4.5,4,4.275,MEDIAN,
,team-topic-modeling,Fred Trotter,3,3,2,3,3,3,2.85,Basic spacey analysis. Well done.,
,team-topic-modeling,Ben Coleman,3,1,1,1,1,1,1.4,"No documentation, and all code is in one file. It uses spacey to do basic analysis. He doesn't see anything new here.",
,team-topic-modeling,Santhosh kumar Veeramalla,2,1.5,1.5,1.5,1.5,1,1.53,No clear documentation. The code lacks clarity and structure; it should be cleaned up and written more clearly to enhance readability and maintainability.,
,team-topic-modeling,Gautami Nadkarni,4,4.5,5,3,5,5,4.4,"This project is an exceptional and highly relevant application of NLP, effectively tackling the complex task of regulatory document analysis. The team's specialized pipeline for extracting regulatory sentences and generating clear, structured insights demonstrates both strong novelty and a deep understanding of user needs.",
,team-topic-modeling,Michael Deeb,3,2,2,2,2,2,2.2,"Leverages standard NLP (spaCy, RAKE) to highlight regulatory topics and auto-generate visual insights; useful for quick exploratory analysis. Code runs but is single-file with hard-coded paths, minimal documentation.",
Total,team-topic-modeling,,3,2,2,2,2,2,2.2,MEDIAN,
,the-scrapers,Fred Trotter,5,5,4,3,3,4,4.1,The possibility that mirrulations could expand beyond regulations.gov is mind blowing.,
,the-scrapers,Ben Coleman,4,4,3,2,2,4,3.25,"There was limited documentation, which is unfortunate, as this project produced valuable insights to the broader mirrulations ecosystem. Understanding that the SEC does NOT have an API and the FCC DOES is valuable if they decide to expand the data beyond regulations.gov. The work uncovered an attempt by the SEC to block scrapers; while frustrating during the event, understanding this space is a valuable contribution (he wishes it had been documented). Similarly, he wants to understand the format of the FCC data more so they can contemplate its compatibility with the current bucket structure.",
,the-scrapers,Santhosh kumar Veeramalla,4,4,3.5,4,4,3.5,3.85,"A solid initiative, but it would be great if it included a note on scraping ethics and compliance. It serves as a starter kit for building more robust government data pipelines.",
,the-scrapers,Evan Tung,5,3,4,2,2,3,3.25,"This was definitely a high-impact project in that they now have the possibility to add non-Regulations.gov data sources to the Mirrulations dataset. Figuring out how the SEC and FCC APIs/websites work in order to scrape them will definitely help in the future. However, he would have liked to see better Open Source Practices with more instructions on how to use the project.",
,the-scrapers,Gautami Nadkarni,2,2,2.5,3,3,3,2.53,"This project is a fantastic contribution to the civic tech community, providing a practical and well-documented resource for scraping government websites. Its strength lies not only in the code but also in its focus on sharing methodologies and challenges, which significantly lowers the barrier to entry for others.",
,the-scrapers,Michael Deeb,5,3,4,3,3,2,3.4,"Leverages official FCC API and a reusable CSS‐selector scraper to retrieve filings and documents; addresses the “external agency scraping” challenge directly; straightforward Python code that newcomers can run with minimal dependencies. Code lacks packaging, tests, and robust error handling; SEC scraping is still manual and incomplete; documentation needs expansion. Huge impact and amplification affects.",
Total,the-scrapers,,4.5,3.5,3.75,3,3,3.25,3.325,MEDIAN,
,within-docket-dataset,Fred Trotter,3,4,4,3,3,5,3.65,"This project taught him the ""obvious after you say it"" way to solve this problem. He doubts the code will be useful. But the idea and concept is solid gold and really impacted how he thinks about the problem.",
,within-docket-dataset,Ben Coleman,3,2,3,2,2,3,2.5,"This project aims to identify changes in proposed rules (i.e., diff between V1 and V2 of a proposal) and then find comments that influenced this change, which would be incredibly valuable but is incredibly hard to accomplish. Steps are documented at a high level, but no technical details are included. From a scan of the Jupyter notebook, he doesn't see how they are identifying changes to the proposal. It's a great idea, but he doesn't see a lot of delivered results addressing the significant challenges in this space.",
,within-docket-dataset,Gautami Nadkarni,4,3.5,4,4,4,3.5,3.83,Great architecture and use of AI for impact.,
,within-docket-dataset,Melanie Kourbage,5,3,5,5,2,2,3.7,"The initial process flow looks promising, and she looks forward to seeing more as the team continues to build out this tool. Figuring out the impact of the comments on the final rule is so valuable. It can take a lot of subject matter experts a lot of time to put together thoughtful comments, and she questions if this is a valuable use of their time or if it makes a difference. They have public policy and government affairs staff working on building relationships with these agencies, but being able to study which comments are making an impact and how they're framed would be invaluable.",
,within-docket-dataset,Michael Deeb,4,2,3,2,2,2,2.55,Links comments to rule documents within a docket using timestamp windows and basic text matching. Clear notebook showcases end-to-end pipeline and yields actionable insights for policymakers. Heuristics are sensible but not innovative. Strength lies in demonstrating a replicable analytic workflow that can be expanded with richer NLP.,
Total,within-docket-dataset,,4,3,4,3,2,3,3.65,MEDIAN,
,expanded-search,Ben Coleman,,,,,,,0,This project was not presented at the event. It looks like a search front end,similar to what Moravian did during the Spring 2025 semester.
,expanded-search,Santhosh kumar Veeramalla,4,,,,,3.5,1.33,"It has clear documentation and quality code but is not fully ready. The team appears to be still working on it and did not present at the event, which is why he hasn't rated it across all categories.",
,expanded-search,Gautami Nadkarni,4,3,3,4,4,3.5,3.58,High quality code and implementation.,
,expanded-search,Michael Deeb,3,1,2,3,2,1,2,"Solid start on a docket discovery tool with a FastAPI backend, SQLAlchemy/Alembic data layer, and Angular Material UI. Implementation is incomplete, limited NLP, and no live demo. Documentation and modular design are good foundations.",
Total,expanded-search,,4,2,2.5,3.5,3,3.5,1.665,MEDIAN,
,rules-talk,Ben Coleman,4,4,3,3,4,3,3.55,"It processes the HTM file, something they haven't worked with yet. Using LLMs to perform a type of sentiment analysis is an interesting approach. He doesn't know a lot about this but wonders how reliable it will be. It certainly could be a way to identify ideas within a docket to explore further (i.e., the LLM finds a pattern, and then a human goes in to explore in more detail).",
,rules-talk,Santhosh kumar Veeramalla,4,4,4.5,3.5,4,4,4,Clear documentation and quality code. High impact use case as public comment analysis is often a time-consuming process. Can be further enhanced to compare the results with other LLMs for bias checking and robustness.,
,rules-talk,Melanie Kourbage,4,4,4,5,5,3,4.15,"Provides information on sentiment and ""flavor"" of comments, serving as a pulse check to see if most people are in favor of a rule or not. The instructions for how to use the tool are very detailed, but as a non-technical user, she would likely still rely on one of the programmers on her team to pull this info for her, though the lift would be minimal with these instructions. She would like the report to include who submitted the comments and to tally how many positive and negative comments there are. National Provider Directory is a great first use case.",
,rules-talk,Gautami Nadkarni,5,4.5,5,4,4,5,4.6,"This project represents an exceptional and timely application of AI to a critical government function. By intelligently leveraging the Gemini API to streamline the analysis of policy comments, the team has created a solution with high impact and strong novelty.",
,rules-talk,Taylor Wilson,3,4,5,3,5,4,3.95,"She thought the interface really took this project to the next level. She thinks there's a lot more that can be done with this to make it even more useful. She believes the current implementation won't have too much impact because of how the groupings were created. She thinks trying to use LLMs as a way to topic model a corpus is a great use case, but it does require passing large parts of that corpus to the models, which can be challenging if the corpus is too large or runs into issues with the context windows of your target models. One could leverage a ""catch all"" category if you know you want to capture certain types of comments and don't care too much about ones that deviate from that space. User-centered design principles can be very useful in helping take this to the next level. It might be better to create known classification frameworks for comment types then have the LLM bucket comments into that pre-existing framework; this requires a lot of domain knowledge and upfront work, but once it gets going it is extremely powerful. She thanks them for their hard work.",
,rules-talk,Michael Deeb,4,3,3,3,3,3,3.2,"Leverages LLM (Gemini) to auto-outline policies and cluster comment issues with sentiment, delivering high-value insights for rule-makers. Clear JSON schemas, CLI, and docs aid reproducibility.",
Total,rules-talk,,4,4,4.25,3.25,4,3.5,3.975,MEDIAN,
,taskmasters,Ben Coleman,3,3,3,2,2,2,2.55,"Minimal documentation. Main folder and sub-folders appear to be different sub-projects, not unified. Work on docx is interesting—something they haven't considered yet but want to. Mix of Jupyter and straight Python is hard to synthesize.",
,taskmasters,Gautami Nadkarni,4,4.5,5,3,4,5,4.25,"This project provides a robust and well-designed solution to the common problem of inconsistent and unstructured data. By combining multi-format data extraction with keyword analysis and an efficient data storage format, the team has created a highly impactful and usable tool with strong continuity potential.",
,taskmasters,Santhosh kumar Veeramalla,3,3,2.5,3,3,3.5,3,Handling PDF images and text files enhances data coverage. Transforming JSON to parquet and leveraging AWS Glue & Athena aligns with best practice. Minor enhancements like deduplication and robust error handling can further strengthen the solution. Clear documentation explaining the purpose of each folder and file would greatly enhance usability and understanding.,
,taskmasters,Michael Deeb,4,3,3,2,2,3,2.9,"Provides a practical pipeline to extract text from PDFs, DOCX, and images, then clean and convert to structured CSV/Parquet for faster analysis. Novel DOCX parsing. Addresses the data-quality problem statement directly and could slot into other teams’ ETL workflows, but techniques are standard OCR/NLP without novel algorithms. Repo offers example scripts and requirements files.",
Total,taskmasters,,3.5,3,3,2.5,2.5,3.25,2.95,MEDIAN,
,team-velogear,Ben Coleman,1,1,1,4,3,1,1.75,"It is a tool to process 1 PDF file at a time. There is no integration with mirrulations concepts. It's just a wrapper around a PDF tool. It was a well-presented project, but it doesn't advance the mirrulations ecosystem.",
,team-velogear,Gautami Nadkarni,3,3,2.5,3,3,3,2.93,"This project is a practical and well-executed utility that fills a specific need for data format conversion from PDFs. While it offers limited novelty by leveraging an existing tool, its design as a command-line utility in Go gives it strong potential for integration into other projects and data pipelines.",
,team-velogear,Michael Deeb,3,1,2,2,3,2,2.15,"Simple Go CLI that leverages `pdftotext` to convert PDFs into CSV/JSON/Parquet, lowering barriers to analyzing regulatory documents. Clear README and functional code.",
Total,team-velogear,,3,1,2,3,3,2,2.15,MEDIAN,
,uspf1,Ben Coleman,2,2,2,2,2,2,2,"Tag generation for dockets would be incredibly valuable for the mirrulations ecosystem. The source code doesn't seem to match the description or the results in the README; the code seems to use an LLM to determine if text is from a letter or to get ""specific information requested"". The results in the README don't look like tags that expand what information can be informed from the organization and title of the docket.",
,uspf1,Gautami Nadkarni,2,2,3,4,5,3,3.05,"This project is a highly impactful and novel solution that directly addresses a critical need for pharmaceutical manufacturers and researchers. By applying an automated classification system to FDA docket comments, the team has created a highly usable tool that provides actionable insights. The project's focused design gives it strong potential for continued development and relevance in the regulatory space.",
,uspf1,Michael Deeb,3,3,3,2,2,2,2.55,A LLM analysis pipeline that builds out some basic bones for doing more complicated analysis.,
Total,uspf1,,2,2,3,2,2,2,2.55,MEDIAN,